1/12/2020
- overfitting is still a HUGE issue (not terrible)
- val_accuracy is around 77, would like to bring it up
- will try dropout
- added EarlyStopping w/ patience = 3
- added Dropout to handle overfitting, similar results between 0.4 and 0.5
- will try new method w/o converting dataframes to datasets [tomorrow]

1/11/2020
- Formula of mean of the words by ponderation w/ their Tf-idf:
    n = # of words in text
    Wi = vector Word2Vec (size 300) for a given word i
    Ti is the value of tfidf for a given word i
- my word2vec model performs way worse than tensorflow_RNN (accuracy was ~50, yikes)
- ultimately decided to focus on fine tuning tensorflow_RNN


1/9/2020
- working on manual_tokenized (name might change)
- word2vec - use twitter or google news model?
- might just test both to see which performs better
- text_processing - considered just removing contractions, commented out original
- Tfid is a good way to measure the strength of a word in a document

1/5/2020
- finished (?) text_processing to fully preprocess text
    "6205 - I want smoke"
- For our training Word2Vec embedding:
    sentences = list of sentences (tweets)
    size = # of dimensions we want to represent our word (size of the word vector)
    min_count = words w/ freq > min_count are only going to be included
    window = only terms that occur w/in a window-neighborhood of a term are associated (?) ; usually 4-5
    workers = # of threads used in training parallelization (i.e. speeds it up)
- renamed file, but code still does not work


1/4/2020
- from reading online, I must use the same tokenizer in training and test data, otherwise
    there will be different tokens for each dataset
- batch size = # of sample processed before model is updated:
    >= 1 OR <= # of samples in training dataset
- # of epochs = # of complete passes through the dataset (arbitrary, can be infinite)
- epochs * steps per epoch = batch size
- will need to improve tf.dataset pipeline?
- need to figure out what happened when I converted df to ds (why do I go from 4872 to 49???)
- will try using tf.hub on separate python file
- also try to work with creating an input function [that returns a tf.data.Dataset object]?


1/3/2020
- reformatted log so most recent is at top
- for tf.hub, will work on just one feature column (text), eventually will add keywords and location
- NOTE: CANNOT use tensorflow_text because not release for Windows (big sad)
- Maybe I should try tokenization before splitting for tensorflow_RNN (using tf's Tokenizer)
- Must experiment with:
    Buffer/Batch Size
    Activation Functions
    Epsilon hyperparameter for adam (no idea why official TF guide uses 1e-4)
- What are the optimal hyperparamters for NLP? (maybe to generic question)
- [IMPORTANT] Following warnings can be ignored (per official tickets from TF's github):
    "Skipping optimization due to error while loading function libraries: Invalid argument"
    "BaseCollectiveExecutor::StartAbort Out of range: End of sequence"
    - TF 2.1 will (apparently) fix these warnings


1/2/2020
- Should I use tensorflow_dataset tools?
- Created tensorflow_RNN to test the above (so I don't overwrite what I have)
- HUGE PROBLEM: How do I set aside sub datasets for training and evaluating?
    1. train_test_split for test, train, val then load into tf.Dataset?
    2. load into tf.Dataset, then split that Dataset?
    Decided to go with 1, par val (so jus test and train)
- ANOTHER HUGE PROBLEM: Need corpus and vocab to actually decode, might just go back to disaster.py
- Had to nltk.download: stopwords, wordnet
- for now, will not normalize text (i.e. dates, numbers, abbreviations -> text)
- probably will need to replace nltk lemmatization w/ spacy, but nltk good for now
- forgot about tensorflow_hub, will try to implement in tensorflow_RNN


1/1/2020
- How do I expand contractions? (i.e. What's -> What is)
- I should probably:
    1. Tokenize
    2. Expand contractions
    3. Eliminate punctuation par #
    4. Lowercase all?
- nltk vs spacy? -> gonna go w/ nltk
