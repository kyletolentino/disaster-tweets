1/4/2020
- from reading online, I must use the same tokenizer in training and test data, otherwise
    there will be different tokens for each dataset
- batch size = # of sample processed before model is updated:
    >= 1 OR <= # of samples in training dataset
- # of epochs = # of complete passes through the dataset (arbitrary, can be infinite)
- epochs * steps per epoch = batch size
- will need to improve tf.dataset pipeline?
- need to figure out what happened when I converted df to ds (why do I go from 4872 to 49???)
- will try using tf.hub on separate python file



1/3/2020
- reformatted log so most recent is at top
- for tf.hub, will work on just one feature column (text), eventually will add keywords and location
- NOTE: CANNOT use tensorflow_text because not release for Windows (big sad)
- Maybe I should try tokenization before splitting for tensorflow_RNN (using tf's Tokenizer)
- Must experiment with:
    Buffer/Batch Size
    Activation Functions
    Epsilon hyperparameter for adam (no idea why official TF guide uses 1e-4)
- What are the optimal hyperparamters for NLP? (maybe to generic question)
- [IMPORTANT] Following warnings can be ignored (per official tickets from TF's github):
    "Skipping optimization due to error while loading function libraries: Invalid argument"
    "BaseCollectiveExecutor::StartAbort Out of range: End of sequence"
    - TF 2.1 will (apparently) fix these warnings


1/2/2020
- Should I use tensorflow_dataset tools?
- Created tensorflow_RNN to test the above (so I don't overwrite what I have)
- HUGE PROBLEM: How do I set aside sub datasets for training and evaluating?
    1. train_test_split for test, train, val then load into tf.Dataset?
    2. load into tf.Dataset, then split that Dataset?
    Decided to go with 1, par val (so jus test and train)
- ANOTHER HUGE PROBLEM: Need corpus and vocab to actually decode, might just go back to disaster.py
- Had to nltk.download: stopwords, wordnet
- for now, will not normalize text (i.e. dates, numbers, abbreviations -> text)
- probably will need to replace nltk lemmatization w/ spacy, but nltk good for now
- forgot about tensorflow_hub, will try to implement in tensorflow_RNN


1/1/2020
- How do I expand contractions? (i.e. What's -> What is)
- I should probably:
    1. Tokenize
    2. Expand contractions
    3. Eliminate punctuation par #
    4. Lowercase all?
- nltk vs spacy? -> gonna go w/ nltk
